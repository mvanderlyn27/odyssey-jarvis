Of course. Here is a detailed, step-by-step implementation plan for your coding LLM. It is designed to be executed in order, providing the necessary SQL commands for a Supabase environment to transition your analytics to an hourly aggregation model.

Subject: Implementation Plan for TikTok Analytics Database Optimization

Objective: Transition the current 10-minute granularity post_analytics database to an hourly aggregation model to improve performance and manage data volume. This plan involves creating a new hourly summary table, automating data aggregation and cleanup, and optimizing queries.

Step 1: Create the Hourly Aggregation Table

The first step is to create a new table, post_analytics_hourly, that will store the aggregated data. This table will be the primary source for analytics queries beyond the most recent few hours.

Action: Execute the following SQL in your Supabase SQL Editor.

```sql
-- Create a table to hold aggregated hourly analytics data.
CREATE TABLE public.post_analytics_hourly (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    post_id TEXT NOT NULL,
    hour TIMESTAMPTZ NOT NULL,
    views_latest BIGINT,
    likes_latest BIGINT,
    comments_latest BIGINT,
    shares_latest BIGINT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Create a unique constraint to prevent duplicate entries for the same post in the same hour.
-- This is crucial for the "upsert" logic in the next step.
ALTER TABLE public.post_analytics_hourly
ADD CONSTRAINT unique_post_hour UNIQUE (post_id, hour);
```
Step 2: Create a Database Function for Hourly Aggregation

This PostgreSQL function will contain the logic to read the recent granular data from post_analytics, calculate the latest values for each hour, and insert or update them into the post_analytics_hourly table.

Action: Create a new database function using the following SQL.

```sql
-- This function aggregates the latest stats for each post from the last 2 hours
-- and "upserts" them into the post_analytics_hourly table.
CREATE OR REPLACE FUNCTION aggregate_hourly_analytics()
RETURNS void AS $$
BEGIN
  -- We use INSERT ... ON CONFLICT (upsert) to create or update hourly records.
  INSERT INTO public.post_analytics_hourly (hour, post_id, views_latest, likes_latest, comments_latest, shares_latest)
  SELECT
    -- Truncate the timestamp to the beginning of the hour.
    date_trunc('hour', pa.timestamp) AS hour,
    pa.post_id,
    -- Use MAX() to get the latest (highest) count recorded within that hour.
    MAX(pa.views) AS views_latest,
    MAX(pa.likes) AS likes_latest,
    MAX(pa.comments) AS comments_latest,
    MAX(pa.shares) AS shares_latest
  FROM
    public.post_analytics pa
  WHERE
    -- Process data from the last 2 hours to ensure all data for the previous hour is captured.
    pa.timestamp >= date_trunc('hour', NOW() - INTERVAL '2 hours')
  GROUP BY
    hour, pa.post_id
  -- If a record for a post and hour already exists, update it with the new latest values.
  ON CONFLICT (post_id, hour) DO UPDATE
  SET
    views_latest = EXCLUDED.views_latest,
    likes_latest = EXCLUDED.likes_latest,
    comments_latest = EXCLUDED.comments_latest,
    shares_latest = EXCLUDED.shares_latest;
END;
$$ LANGUAGE plpgsql;
```
Step 3: Automate the Aggregation with a Cron Job

Use pg_cron to schedule the aggregate_hourly_analytics function to run automatically. We will run it every hour, 5 minutes past the hour, to ensure all data from the previous hour has been collected.

Action: Enable the pg_cron extension and schedule the job.

```sql
-- 1. Enable the pg_cron extension if you haven't already.
CREATE EXTENSION IF NOT EXISTS pg_cron WITH SCHEMA extensions;

-- 2. Schedule the aggregation function to run at 5 minutes past every hour.
-- Format: 'minute hour day month day-of-week'
SELECT cron.schedule(
  'hourly-analytics-aggregation',
  '5 * * * *', -- This means "at minute 5 of every hour"
  $$
  SELECT public.aggregate_hourly_analytics();
  $$
);
```

### **Step 4: Implement a Data Retention Policy (Downsampling)**

To keep the `post_analytics` table small and fast, create a function to delete the raw, 10-minute data after it's no longer needed for immediate analysis (e.g., older than 2 days).

**Action:** Create the cleanup function and schedule it to run daily.

```sql
-- 1. Create the function to delete old granular data.
CREATE OR REPLACE FUNCTION delete_old_granular_analytics()
RETURNS void AS $$
BEGIN
  -- Delete records from the high-frequency table older than 2 days.
  DELETE FROM public.post_analytics
  WHERE timestamp < NOW() - INTERVAL '2 days';
END;
$$ LANGUAGE plpgsql;

-- 2. Schedule this cleanup function to run once per day (e.g., at 3:00 AM UTC).
SELECT cron.schedule(
  'delete-old-granular-analytics',
  '0 3 * * *', -- Runs every day at 3:00 AM UTC
  $$
  SELECT public.delete_old_granular_analytics();
  $$
);
```
Step 5: Add Indexes for Query Performance

Ensure both tables are properly indexed to make data retrieval fast. This is the most critical step for performance.

Action: Execute the following CREATE INDEX commands.

```sql
-- Index for the new hourly table, for querying by post over time.
CREATE INDEX idx_hourly_post_id_hour ON public.post_analytics_hourly (post_id, hour DESC);

-- Index for the hourly table, for querying general trends across all posts.
CREATE INDEX idx_hourly_hour ON public.post_analytics_hourly (hour DESC);

-- Index for the original granular table, to speed up aggregation and real-time queries.
CREATE INDEX idx_post_analytics_timestamp ON public.post_analytics (timestamp DESC);
CREATE INDEX idx_post_analytics_post_id_timestamp ON public.post_analytics (post_id, timestamp DESC);
```
Step 6: Update Application Query Logic

Your application will now need to query the appropriate table based on the required data granularity.

Action: Instruct the LLM to modify application code according to the following logic:

For highly recent, real-time data (e.g., last 2 hours): Query the original post_analytics table.

For hourly analytics, daily summaries, or historical trends: Query the post_analytics_hourly table. This should be used for most charts and analytics.

Example Query: Get daily views for a specific post

```sql
SELECT
  date_trunc('day', hour)::date AS day,
  SUM(views_latest) AS total_daily_views
FROM
  public.post_analytics_hourly
WHERE
  post_id = 'your_specific_post_id'
GROUP BY
  day
ORDER BY
  day DESC;
```
Example Query: Get total views across all accounts in the last 24 hours

```sql
SELECT SUM(views_latest) AS total_views_last_24_hours
FROM public.post_analytics_hourly
WHERE hour >= NOW() - INTERVAL '24 hours';
